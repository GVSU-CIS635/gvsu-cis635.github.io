# Midterm Exam Answer

---

**1.** Which step in the knowledge discovery process involves removing noise and inconsistent data?

- [x] Data Cleaning
- [ ] Data Integration
- [ ] Data Selection
- [ ] Pattern Evaluation

**2.** The attribute type that represents ranked categories is:

- [ ] Continuous
- [x] Ordinal
- [ ] Numeric
- [ ] Binary

**3.** Scatter plots are primarily used to visualize:

- [ ] Distributions
- [x] Correlations
- [ ] Central Tendencies
- [ ] Variance

**4.** If you intend to forecast a specific numeric value based on input data, you should employ:

- [ ] Dimensionality reduction
- [ ] Classification
- [ ] Clustering
- [x] Regression

**5.** Which method is not related to dimensionality reduction?

- [ ] Feature Selection
- [ ] Principal Component Analysis (PCA)
- [x] Information Gain
- [ ] Stochastic Neighbor Embedding (SNE)

**6.** (Bonus) You created a learning system that interacts with its environment and responds to errors and rewards. What type of machine learning system is it?

- [ ] Supervised learning
- [ ] Semi-supervised learning
- [x] Reinforcement learning
- [ ] Unsupervised learning

**7.** The mean and median are measures of dispersion.

- [ ] True
- [x] False

**8.** A histogram is used to display a single variable's distribution.

- [x] True
- [ ] False

**9.** The KL divergence measures how one probability distribution differs from a second.

- [x] True
- [ ] False

**10.** Normalization involves adjusting the scales of features.

- [x] True
- [ ] False

**11.** The curse of dimensionality refers to the challenge that arises with an increase in the dimensionality of data.

- [x] True
- [ ] False

**12.** In unsupervised learning, the algorithm is trained on labeled training data.

- [ ] True
- [x] False

**13.** To minimize the information required to classify the tuples in the resulting partitions and reflect the least randomness or "impurity" in these partitions, the attribute with the lowest information gain should be chosen as the splitting attribute.

- [ ] True
- [x] False

**14.** (Bonus) PCA is a supervised learning.

- [ ] True
- [x] False

---

**15.** A realtor is leveraging historical data to better predict the ease of selling houses. The dataset below classifies houses based on whether they were easily sold or not:

| House | Price | Area (sqft) | Easy to Sell? |
| ----- | ----- | ----------- | ------------- |
| 1     | $320k | 1200        | Yes           |
| 2     | $360k | 1350        | Yes           |
| 3     | $520k | 3000        | No            |
| 4     | $720k | 4200        | No            |

Considering a new property, house 5, with a price of \$400k and an area of 2700 sqft, predict whether it will be easily sold. Using the Euclidean distance formula, base your prediction on the most similar house from the historical dataset.

a. Find the most similar house from the historical dataset using the Euclidean distance.

```{note}
Given the differing scales of attributes (price and area), it's essential to apply min-max normalization before computing the distance.
```

- **Min-max normalization:**
  - Price: max - min = 720 - 320 = 400
    - Normalized price:
      - House 1 = $\frac{320 - 320}{400} = 0$;
      - House 2 = $\frac{360-320}{400} = 0.1$;
      - House 3 = $\frac{520-320}{400} = 0.5$;
      - House 4 = $\frac{720-320}{400} = 1$;
      - House 5 = $\frac{400-320}{400} = 0.2$
  - Area: max - min = 4200 - 1200 = 3000
    - Normalized price:
      - House 1 = $\frac{1200 - 1200}{3000} = 0$;
      - House 2 = $\frac{1350 - 1200}{3000} = 0.05$;
      - House 3 = $\frac{3000 - 1200}{3000} = 0.6$;
      - House 4 = $\frac{4200 - 1200}{3000} = 1$;
      - House 5 = $\frac{2700 - 1200}{3000} = 0.5$
- **Dissimilarity Calculation using `Euclidean Distance (L2 norm)`**

  $$d(i, j) = \sqrt{\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}$$

  | House | Normalized Price | Normalized Area | Distance to House 5 (0.2, 0.5) |
  | --- | --- | --- | --- |
  | 1 | 0 | 0 | $\sqrt{0.2^2+0.5^2} = 0.5385$ |
  | 2 | 0.1 | 0.05 | $\sqrt{(0.2-0.1)^2+(0.5-0.05)^2} = =0.461$ |
  | 3 | 0.5 | 0.6 | $\sqrt{(0.2-0.5)^2+(0.5-0.6)^2} = 0.3162$ |
  | 4 | 1 | 1 | $\sqrt{(0.2-1)^2+(0.5-1)^2} = 0.9434$ |

From the above table, the most similar house to House 5 is House 3.

b. (Bonus) Based on your previous analysis, is House 5 easy to sell?

- [ ] Yes
- [x] No

```{hint}
:class: warning
House 5 is most similar to House 3, and since House 3 was not easily sold, it suggests that House 5 may also not be easy to sell.
```

**16.** We conducted a survey of 100 attendees at our theater to ascertain their viewing and purchasing behaviors. For each attendee, we recorded the movie genre they watched and whether they bought snacks. The goal of this survey is to determine if there is a relationship between the movie genre and the likelihood of purchasing snacks. Consider the two variables: `movie_genre` and `snack_purchase`.

|        | Snacks Purchased | No Snacks Purchased |     |
| ------ | ---------------- | ------------------- | --- |
| Action | a 20 (30)        | b 40 (30)           | 60  |
| Comedy | c 30 (20)        | d 10 (20)           | 40  |
|        | 50               | 50                  | 100 |

a. Calculate the χ² statistic for the given data.

- solution 1:

  $$
  \chi^2 = \frac{(20-30)^2}{30}+\frac{(30-20)^2}{20}+\frac{(40-30)^2}{30}+\frac{(10-20)^2}{20} = 16.667
  $$

- solution 2:

  $$
  \chi^2 = \frac{(ad-bc)^2(a+b+c+d)}{(a+b)(c+d)(b+d)(a+c)}=\frac{(200-1200)^2\times 100}{60\times 40\times 50\times 50} = 16.667
  $$

b. (Bonus) Given the following critical values for the χ² distribution, and selecting 0.001 as the significance level, can we infer that the variables `movie_genre` and `snack_purchase` are related?

- [x] Yes
- [ ] No

---

**17.** Imagine you're building a decision tree to predict whether or not people will play tennis based on weather conditions. The dataset consists of two features: WindStrength (with values "Weak" or "Strong") and Temperature (with values "Hot" or "Mild"). The target variable, PlayTennis, has two classes: "Yes" (indicating individuals will play tennis) and "No" (indicating they won't).

a. Calculate the `InfoGain(WindStrength)`.

$$
0.94 - (\frac{40}{55}\times 0.88 + \frac{15}{55}\times 0.97) \approx 0.035
$$

b. Calculate the `InfoGain(Temperature)`.

$$
0.94 - (\frac{20}{55}\times 0.48 + \frac{35}{55}\times 0.68) \approx 0.33
$$

c. Based on the calculated Information Gain values, sketch the root node of the decision tree and indicate the best partitioning feature and its partitions.

```bash
Temperature
|-- Hot
|-- Mild
```

d.(Bonus) Which of the following statements about information gain is/are true? Choose more than one answer if necessary.

- [ ] Information gain will always be a positive value.
- [x] Information gain measures the reduction in entropy achieved by partitioning a set.
- [ ] A feature with a high entropy will always have a high information gain.
- [x] A higher information gain indicates a more informative feature for splitting the dataset.
